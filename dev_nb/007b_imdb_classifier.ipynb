{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_007a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been prepared in csv files at the beginning 007a, we will use it know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/aclImdb/')\n",
    "CLAS_PATH = PATH/'clas'\n",
    "LM_PATH = PATH/'lm'\n",
    "MODEL_PATH = PATH/'models'\n",
    "os.makedirs(CLAS_PATH, exist_ok=True)\n",
    "os.makedirs(LM_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(rules=rules, special_cases=[BOS, FLD, UNK, PAD])\n",
    "train_ds, valid_ds = TextDataset.from_csv(LM_PATH, tokenizer, max_vocab=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 50,70\n",
    "train_dl = LanguageModelLoader(np.concatenate(train_ds.ids), bs, bptt)\n",
    "valid_dl = LanguageModelLoader(np.concatenate(valid_ds.ids), bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt the pre-trained weights to the new vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pretrained model and the corresponding itos dictionary here and put them in the MODEL_PATH folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_wt = pickle.load(open(MODEL_PATH/'itos4.pkl', 'rb'))\n",
    "stoi_wt = {v:k for k,v in enumerate(itos_wt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert_weights(wgts, stoi_wgts, itos_new):\n",
    "    dec_bias, enc_wgts = wgts['1.decoder.bias'], wgts['0.encoder.weight']\n",
    "    bias_m, wgts_m = dec_bias.mean(0), enc_wgts.mean(0)\n",
    "    new_w = enc_wgts.new_zeros((len(itos_new),enc_wgts.size(1))).zero_()\n",
    "    new_b = dec_bias.new_zeros((len(itos_new),)).zero_()\n",
    "    for i,w in enumerate(itos_new):\n",
    "        r = stoi_wgts[w] if w in stoi_wgts else -1\n",
    "        new_w[i] = enc_wgts[r] if r>=0 else wgts_m\n",
    "        new_b[i] = dec_bias[r] if r>=0 else bias_m\n",
    "    wgts['0.encoder.weight'] = new_w\n",
    "    wgts['0.encoder_dp.emb.weight'] = new_w.clone()\n",
    "    wgts['1.decoder.weight'] = new_w.clone()\n",
    "    wgts['1.decoder.bias'] = new_b\n",
    "    return wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(MODEL_PATH/'lstm4.pth', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['1.decoder.bias'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_wt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = convert_weights(wgts, stoi_wt, train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['1.decoder.bias'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(train_ds.vocab.itos)\n",
    "emb_sz,nh,nl = 400,1150,3\n",
    "dps = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_language_model(vocab_size, emb_sz, nh, nl, 0, input_p=dps[0], output_p=dps[1], weight_p=dps[2], \n",
    "                           embed_p=dps[3], hidden_p=dps[4])\n",
    "model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation in different groups for discriminitative lr and gradual unfreezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [nn.Sequential(rnn, dp) for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)] \n",
    "groups.append(nn.Sequential(model[0].encoder, model[0].encoder_dp, model[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, model)\n",
    "learn.layer_groups = groups\n",
    "learn.callbacks.append(RNNTrainer(learn, bptt, alpha=2, beta=1))\n",
    "learn.metrics = [accuracy]\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7), pct_start=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fine_tuned60kb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fine_tuned60kb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_encoder(learn, name):\n",
    "    torch.save(learn.model[0].state_dict(), learn.path/f'{name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_encoder(learn, 'fine_tuned_enc60kb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use the same itos than the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CLAS_PATH/'tmp', exist_ok=True)\n",
    "shutil.copy(LM_PATH/'tmp'/'itos.pkl', CLAS_PATH/'tmp'/'itos.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(CLAS_PATH/'tmp')\n",
    "tokenizer = Tokenizer(rules=rules, special_cases=[BOS, FLD, UNK, PAD])\n",
    "train_ds, valid_ds = TextDataset.from_csv(CLAS_PATH, tokenizer, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.vocab.itos[:20], vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.vocab.textify(train_ds.ids[1]), train_ds.labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import Sampler, BatchSampler\n",
    "\n",
    "class SortSampler(Sampler):\n",
    "    \"Go through the text data by order of length\"\n",
    "    \n",
    "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(range(len(self.data_source)), key=self.key, reverse=True))\n",
    "\n",
    "\n",
    "class SortishSampler(Sampler):\n",
    "    \"Go through the text data by order of length with a bit of randomness\"\n",
    "    \n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source,self.key,self.bs = data_source,key,bs\n",
    "\n",
    "    def __len__(self): return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = np.random.permutation(len(self.data_source))\n",
    "        sz = self.bs*50\n",
    "        ck_idx = [idxs[i:i+sz] for i in range(0, len(idxs), sz)]\n",
    "        sort_idx = np.concatenate([sorted(s, key=self.key, reverse=True) for s in ck_idx])\n",
    "        sz = self.bs\n",
    "        ck_idx = [sort_idx[i:i+sz] for i in range(0, len(sort_idx), sz)]\n",
    "        max_ck = np.argmax([self.key(ck[0]) for ck in ck_idx])  # find the chunk with the largest key,\n",
    "        ck_idx[0],ck_idx[max_ck] = ck_idx[max_ck],ck_idx[0]     # then make sure it goes first.\n",
    "        sort_idx = np.concatenate(np.random.permutation(ck_idx[1:]))\n",
    "        sort_idx = np.concatenate((ck_idx[0], sort_idx))\n",
    "        return iter(sort_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=True):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(max_len, len(samples)).long() + pad_idx\n",
    "    for i,s in enumerate(samples): res[-len(s[0]):,i] = LongTensor(s[0])\n",
    "    return res, LongTensor([s[1] for s in samples]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SortishSampler(train_ds.ids, key=lambda x: len(train_ds.ids[x]), bs=bs//2)\n",
    "valid_sampler = SortSampler(valid_ds.ids, key=lambda x: len(valid_ds.ids[x]))\n",
    "train_dl = DeviceDataLoader.create(train_ds, bs//2, sampler=train_sampler, collate_fn=pad_collate)\n",
    "valid_dl = DeviceDataLoader.create(valid_ds, bs,  sampler=valid_sampler, collate_fn=pad_collate)\n",
    "data = DataBunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.textify(x[:,15]), y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiBatchRNNCore(RNNCore):\n",
    "    def __init__(self, bptt, max_seq, *args, **kwargs):\n",
    "        self.max_seq,self.bptt = max_seq,bptt\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def concat(self, arrs):\n",
    "        return [torch.cat([l[si] for l in arrs]) for si in range(len(arrs[0]))]\n",
    "\n",
    "    def forward(self, input):\n",
    "        sl,bs = input.size()\n",
    "        self.reset()\n",
    "        raw_outputs, outputs = [],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r, o = super().forward(input[i: min(i+self.bptt, sl)])\n",
    "            if i>(sl-self.max_seq):\n",
    "                raw_outputs.append(r)\n",
    "                outputs.append(o)\n",
    "        return self.concat(raw_outputs), self.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bn_dp_lin(n_in, n_out, drop, relu=True): \n",
    "    layers = [nn.BatchNorm1d(n_in), nn.Dropout(drop), nn.Linear(n_in, n_out)]\n",
    "    if relu: layers.append(nn.ReLU(inplace=True))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PoolingLinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        lyrs = []\n",
    "        for i in range(len(layers)-1):\n",
    "            lyrs += bn_dp_lin(layers[i], layers[i + 1], drops[i], i!=len(layers)-2)\n",
    "        self.layers = nn.Sequential(*lyrs)\n",
    "\n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        x = self.layers(x)\n",
    "        return x, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_rnn_classifier(bptt, max_seq, n_class, vocab_sz, emb_sz, n_hid, n_layers, pad_token, layers, drops, \n",
    "                       bidir=False, qrnn=False, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "    rnn_enc = MultiBatchRNNCore(bptt, max_seq, vocab_sz, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir,\n",
    "                      qrnn=qrnn, hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    return SequentialRNN(rnn_enc, PoolingLinearClassifier(layers, drops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size,n_class = len(train_ds.vocab.itos),2\n",
    "emb_sz,nh,nl = 400,1150,3\n",
    "#dps = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_rnn_classifier(bptt, 20*70, n_class, vocab_size, emb_sz=emb_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[emb_sz*3, 50, n_class], drops=[dps[1], 0.1],\n",
    "          input_p=dps[0], weight_p=dps[2], embed_p=dps[3], hidden_p=dps[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [nn.Sequential(model[0].encoder, model[0].encoder_dp)]\n",
    "groups += [nn.Sequential(rnn, dp) for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)] \n",
    "groups.append(model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, model)\n",
    "learn.layer_groups = groups\n",
    "learn.callbacks.append(RNNTrainer(learn, bptt, alpha=2, beta=1, adjust=False))\n",
    "learn.callback_fns.append(partial(GradientClipping, clip=0.12))\n",
    "learn.metrics = [accuracy]\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_encoder(learn, name):\n",
    "    learn.model[0].load_state_dict(torch.load(learn.path/f'{name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_encoder(learn, 'fine_tuned_enc60ka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, lrs, moms=(0.8,0.7), pct_start=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, lrs, moms=(0.8,0.7), pct_start=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, lrs, moms=(0.8,0.7), pct_start=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
