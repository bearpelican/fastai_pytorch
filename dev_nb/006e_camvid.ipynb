{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_005a import *\n",
    "from nb_005b import *\n",
    "\n",
    "from nb_006 import *\n",
    "import fast_progress as fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camvid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/camvid')\n",
    "PATH_X = PATH/'701_StillsRaw_full'\n",
    "PATH_Y = PATH/'LabeledApproved_full'\n",
    "PATH_Y_PROCESSED = PATH/'LabelProcessed'\n",
    "label_csv = PATH/'label_colors.txt'\n",
    "\n",
    "PATH_Y_PROCESSED.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(PATH_Y.iterdir())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_code(l):\n",
    "    a,b = [c for c in l.strip().split(\"\\t\") if c]\n",
    "    return tuple(int(o) for o in a.split(' ')), b\n",
    "label_codes,label_names = zip(*[parse_code(l) for l in open(PATH/\"label_colors.txt\")])\n",
    "label_codes,label_names = list(label_codes),list(label_names)\n",
    "code2id = {v:k for k,v in enumerate(label_codes)}\n",
    "failed_code = len(label_codes)+1\n",
    "label_codes.append((0,0,0))\n",
    "label_names.append('unk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_fn(x_fn): return PATH_Y/f'{x_fn.name[:-4]}_L.png'\n",
    "def get_y_proc_fn(y_fn): return PATH_Y_PROCESSED/f'{y_fn.name[:-6]}_P.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fns = [o for o in PATH_X.iterdir() if o.is_file()]\n",
    "y_fns = [get_y_fn(o) for o in x_fns]\n",
    "y_proc_fns = [get_y_proc_fn(o) for o in y_fns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fns):\n",
    "    yfn, pfn = fns\n",
    "    if not pfn.exists():\n",
    "        y_data = open_mask(yfn).px.long()\n",
    "\n",
    "        h, w = y_data.shape[1:3]\n",
    "        data = y_data.view(3, -1)\n",
    "        n_pixels = data.shape[1]\n",
    "        proc_data = np.zeros((1, n_pixels),dtype=np.uint8)\n",
    "        for i in range(n_pixels):\n",
    "            proc_data[:,i] = code2id.get(tuple(data[:,i].numpy()), 0)\n",
    "        proc_data.resize((1, h, w))\n",
    "        img = PIL.Image.fromarray(proc_data[0])\n",
    "        img.save(pfn)\n",
    "    return pfn\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "def process_label_files(y_fns, y_proc_fns):\n",
    "    ex = ProcessPoolExecutor(16)\n",
    "    for pfn in ex.map(process_file, zip(y_fns, y_proc_fns)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time process_label_files(y_fns, y_proc_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(path, valid_pct=0.2):\n",
    "    x_fns = [o for o in path.iterdir() if o.is_file()]\n",
    "    y_fns = [get_y_fn(o) for o in x_fns]\n",
    "    y_proc_fns = [get_y_proc_fn(o) for o in y_fns]\n",
    "    total = len(x_fns)\n",
    "    \n",
    "    is_test = np.random.uniform(size=(len(x_fns),)) < valid_pct\n",
    "    ((val_x,trn_x),(val_y,trn_y)) = split_arrs(is_test, x_fns, y_proc_fns)\n",
    "    return (MatchedFilesDataset(trn_x, trn_y),\n",
    "            MatchedFilesDataset(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfm_datasets(size):\n",
    "    datasets = get_datasets(PATH_X)\n",
    "    tfms = get_transforms(do_flip=True, max_rotate=4, max_lighting=0.2)\n",
    "    return transform_datasets(*datasets, tfms=tfms, tfm_y=True, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm,default_denorm = normalize_funcs(*imagenet_stats)\n",
    "bs = 8\n",
    "size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=True, max_rotate=4, max_lighting=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(size, bs):\n",
    "    return DataBunch.create(*get_tfm_datasets(size), bs=bs, tfms=default_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(size, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data.train_ds[0]\n",
    "x.shape, y.shape, y.data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_channels(m):\n",
    "    for l in flatten_model(m):\n",
    "        if hasattr(l, 'weight'): return l.weight.shape[1]\n",
    "    raise Exception('No weight layer')\n",
    "\n",
    "def model_sizes(m, size=(256,256), full=True):\n",
    "    hooks = hook_outputs(m)\n",
    "    ch_in = in_channels(m)\n",
    "    x = torch.zeros(1,ch_in,*size)\n",
    "    x = m.eval()(x)\n",
    "    res = [o.stored.shape for o in hooks]\n",
    "    if not full: hooks.remove()\n",
    "    return res,x,hooks if full else res\n",
    "\n",
    "def get_sfs_idxs(sizes, last=True):\n",
    "    if last:\n",
    "        feature_szs = [size[-1] for size in sizes]\n",
    "        sfs_idxs = list(np.where(np.array(feature_szs[:-1]) != np.array(feature_szs[1:]))[0])\n",
    "        if feature_szs[0] != feature_szs[1]: sfs_idxs = [0] + sfs_idxs\n",
    "    else: sfs_idxs = list(range(len(sfs)))\n",
    "    return sfs_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in_c, x_in_c, hook):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        ni = up_in_c\n",
    "        self.upconv = conv2d_trans(ni, ni//2) # H, W -> 2H, 2W\n",
    "        ni = ni//2 + x_in_c\n",
    "        self.conv1 = conv2d(ni, ni//2)\n",
    "        ni = ni//2\n",
    "        self.conv2 = conv2d(ni, ni)\n",
    "        self.bn = nn.BatchNorm2d(ni)\n",
    "\n",
    "    def forward(self, up_in):\n",
    "        up_out = self.upconv(up_in)\n",
    "        cat_x = torch.cat([up_out, self.hook.stored], dim=1)\n",
    "        x = F.relu(self.conv1(cat_x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Debugger(nn.Module): \n",
    "    def forward(self,x): \n",
    "        set_trace()\n",
    "        return x\n",
    "\n",
    "class DynamicUnet(nn.Sequential):\n",
    "    def __init__(self, encoder, last=True, n_classes=3):\n",
    "        imsize = (256,256)\n",
    "        sfs_szs,x,self.sfs = model_sizes(encoder, size=imsize)\n",
    "        sfs_idxs = reversed(get_sfs_idxs(sfs_szs, last))\n",
    "        \n",
    "        ni = sfs_szs[-1][1]\n",
    "        middle_conv = nn.Sequential(conv2d_relu(ni, ni*2, bn=True), conv2d_relu(ni*2, ni, bn=True))\n",
    "        x = middle_conv(x)\n",
    "        layers = [encoder, nn.ReLU(), middle_conv]\n",
    "\n",
    "        for idx in sfs_idxs:\n",
    "            up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1])\n",
    "            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[idx])\n",
    "            layers.append(unet_block)\n",
    "            x = unet_block(x)\n",
    "\n",
    "        ni = unet_block.conv2.out_channels\n",
    "        if imsize != sfs_szs[0][-2:]: layers.append(conv2d_trans(ni, ni))\n",
    "        layers.append(conv2d(ni, n_classes, 1))\n",
    "        super().__init__(*layers)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_005b import accuracy_thresh\n",
    "metrics=[accuracy_thresh,dice]\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(pred, target):\n",
    "    return F.cross_entropy(pred, target.squeeze().long())\n",
    "\n",
    "body = create_body(tvm.resnet34(True), 2)\n",
    "model = DynamicUnet(body, n_classes=len(label_codes)).cuda()\n",
    "learn = Learner(data, model, metrics=metrics,\n",
    "                loss_fn=my_loss)\n",
    "learn.split([model[0][6], model[1]])\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, slice(lr), pct_start=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(6, slice(lr), pct_start=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('u0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,py = learn.pred_batch()\n",
    "\n",
    "# for i, ax in enumerate(plt.subplots(4,4,figsize=(10,10))[1].flat):\n",
    "#     show_image(default_denorm(x[i].cpu()), py[i]>0, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(6, slice(lr/100,lr), pct_start=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=640\n",
    "bs = 4\n",
    "learn.data = get_data(size, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(6, slice(lr), pct_start=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
